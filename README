SLURM

  Slurm is used for distributed parallel computing.
  - https://slurm.schedmd.com/overview.html

  This repository is meant as an entry-level introduction to containerized use of Slurm and
  is aimed at current HPC computing-model specialists.
  
  MPI capabilities will be implemented shortly.


TESTING IF THIS STUFF WORKS

  Build and start containers:
    $ docker-compose up -d

  Open interactive terminal in submit-container:
    $ docker exec -it submit bash

  Run test from within submit-container:
    root@submit:/share# ./run_test.sh


RUNNING YOUR OWN STUFF

  Copy your own work-files to the container:
    $ docker cp <file> submit:/share/<destination>

  Open interactive shell in submit-container (again):
    $ docker exec -it submit bash

  Run a job command:
    root@submit:/share# srun -N <number-of-nodes> -n <number-of-tasks> <command>

  Queue a batch-job script:
    root@submit:/share# sbatch -N <number-of-nodes> -n <number-of-tasks> <script>

  See output from batch-job:
    root@submit:/share# ls

  View compute nodes:
    root@submit:/share# sinfo

  View job queue:
    root@submit:/share# squeue

  View variables for scripting parallel workloads:
    root@submit:/share# srun env

  Learn more:
    https://slurm.schedmd.com/quickstart.html


CHANGING COMPUTE NODES

  slurm.conf  - All NodeName=<hostname> entries will be polled. https://slurm.schedmd.com/slurm.conf.html#lbAE


SHARED VOLUMES:
  /etc/munge/ - Contains a shared key for communication between containers.
  /share/     - Work-files that are shared during execution.
